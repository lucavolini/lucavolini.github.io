---
layout: default
---

# **Season 3 Ep. 24**

## Binary Prediction of Smoker Status using Bio-Signals

The task here is using binary classification to predict a patient's smoking status given
information about various other health indicators.

The model is evaluated on area under the ROC curve between the predicted probability and
the observed target.

The achieved score is 0.87403 using an ensemble of Gradient Boosted Trees models with 10^3
trees of maximum depth 7; the hyperparameters have been tuned over multiple cross-validation
iterations.

The main steps are the following:

1. [Import libraries](#import-libraries)
2. [Read data](#read-data)
3. [Preprocess data](#preprocess-data)
4. [Define utilities](#define-utilities)
5. [Build model](#build-model)
6. [Make Predictions](#make-predictions)

#### **Import libraries**

The purpose of this step is setting up the environment.

```python
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
import tensorflow_decision_forests as tfdf
import zipfile
```

<span style="float:right; font-size:70%"><a href="#top">Back to top</a></span>

#### **Read data**

This step involves reading data from source files[^1] into pandas dataframes, splitting it
into train and test sets and eventually enriching the former using an auxiliary data source
including features coming from slightly different distributions.

**_Key takeaways_** : test_df is made of .1 of data; df is for training, enriched with
auxiliary data, and does not include the target that is stored in the smoking variable.

```python
zip_ref = zipfile.ZipFile('playground/3.24/files/playground-series-s3e24.zip')
zip_ref.extractall('playground/3.24/files/playground-series-s3e24')
zip_ref.close()

# original dataset
zip_ref = zipfile.ZipFile('playground/3.24/files/smoker-status-prediction-using-biosignals.zip')
zip_ref.extractall('playground/3.24/files/smoker-status-prediction-using-biosignals')
zip_ref.close()

# read data
train_file = 'playground/3.24/files/playground-series-s3e24/train.csv'
df = pd.read_csv(train_file)

# test set
threshold = int(df.shape[0] * 0.9)

test_df = df.iloc[threshold:]
df = df.iloc[:threshold]

test_smoker_id = test_df.pop('id')
test_smoking = test_df.pop('smoking')

# tf.data
test_ds = tf.data.Dataset.from_tensor_slices((dict(test_df), test_smoking))

test_ds = test_ds.batch(batch_size=32)
test_ds = test_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)

# enrich df with original data
original_train_file = 'playground/3.24/files/smoker-status-prediction-using-biosignals/train_dataset.csv'
original_df = pd.read_csv(original_train_file)

smoker_id = df.pop('id')
df = pd.concat([df, original_df], ignore_index=True)

smoking = df.pop('smoking')
```

<span style="float:right; font-size:70%"><a href="#top">Back to top</a></span>

#### **Preprocess data**

The aim of this step is preprocessing data and embed the resulting pipeline into the model
itself.

**_Key takeaways_** : Features are treated differently according to their type. The output
of this step is a preprocessor built using Keras Functional API that normalizes numeric
features, one-hot encodes categorical ones and convert heterogeneous data types to
homogeneous float32.

```python
binary_features = [
    'dental caries'
]
numeric_features = [
    'age',
    'height(cm)',
    'weight(kg)',
    'waist(cm)',
    'eyesight(left)',
    'eyesight(right)',
    'systolic',
    'relaxation',
    'fasting blood sugar',
    'Cholesterol',
    'triglyceride',
    'HDL',
    'LDL',
    'hemoglobin',
    'Urine protein',
    'serum creatinine',
    'AST',
    'ALT',
    'Gtp'
]
categorical_features = [
    'hearing(left)',
    'hearing(right)'
]

# create tf.keras.Input
inputs = {}
for name, column in df.items():
    if name in categorical_features or name in binary_features:
        dtype = tf.int64
    else:
        dtype = tf.float32
    inputs[name] = tf.keras.Input(shape=(), name=name, dtype=dtype)


# sort keys and stack values    
def stack_dict(inp, fun=tf.stack):
    values = []
    for key in sorted(inp.keys()):
        values.append(tf.cast(inp[key], tf.float32))

    return fun(values, axis=-1)


# apply transformations (if necessary)
preprocessed = []

# binary
for name in binary_features:
    preprocessed.append(
        tf.cast(inputs[name][:, tf.newaxis], tf.float32)
    )

# numeric
normalizer = tf.keras.layers.Normalization()
normalizer.adapt(
    stack_dict(dict(df[numeric_features]))
)

numeric_inputs = {}
for name in numeric_features:
    numeric_inputs[name] = inputs[name]

numeric_inputs = stack_dict(numeric_inputs)
numeric_normalized = normalizer(numeric_inputs)

preprocessed.append(numeric_normalized)

# categorical
for name in categorical_features:
    vocab = sorted(set(df[name]))
    lookup = tf.keras.layers.IntegerLookup(vocabulary=vocab, output_mode='one_hot')
    preprocessed.append(
        lookup(inputs[name][:, tf.newaxis])
    )

# preprocessing head
preprocessed_result = tf.concat(preprocessed, axis=-1)
preprocessor = tf.keras.Model(inputs=inputs, outputs=preprocessed_result)
```

<span style="float:right; font-size:70%"><a href="#top">Back to top</a></span>

#### **Define utilities**

The main utilities are those for computing k folds and cross-validating data.
cross_validation function returns the ensemble model.

**_Key takeaways_** : k_fold function splits data into k folds and sets class weights since
target is unbalanced. cross_validation function fits the model to k training and validation
sets and combines models computing the mean.

```python
# utilities


# k-fold cross-validation
def k_fold(d_f, target, pre_processor, k):
    k_fold_dict = {}

    for i in range(k):
        # set validation index range
        fold = 1 / k

        lower_threshold = i * fold
        upper_threshold = lower_threshold + fold

        lower_index = int(d_f.shape[0] * lower_threshold)
        upper_index = int(d_f.shape[0] * upper_threshold)

        boundaries = range(lower_index, upper_index)

        # split df
        train_df = d_f.iloc[lambda x: sorted(set(x.index).difference(boundaries))]
        train_target = target.iloc[lambda x: sorted(set(x.index).difference(boundaries))]

        val_df = d_f.iloc[boundaries]
        val_target = target.iloc[boundaries]

        # set class weights
        neg, pos = np.bincount(train_target)
        total = neg + pos

        weight_for_0 = (1 / neg) * (total / 2.0)
        weight_for_1 = (1 / pos) * (total / 2.0)

        class_weight = {0: weight_for_0, 1: weight_for_1}

        # tf.data
        train_ds = tf.data.Dataset.from_tensor_slices((dict(train_df), train_target))
        val_ds = tf.data.Dataset.from_tensor_slices((dict(val_df), val_target))

        train_ds = train_ds.batch(batch_size=1024)
        val_ds = val_ds.batch(batch_size=1024)

        train_ds = train_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)
        val_ds = val_ds.cache().prefetch(buffer_size=tf.data.AUTOTUNE)

        train_ds_with_preproc = train_ds.map(lambda x, y: (pre_processor(x), y))
        val_ds_with_preproc = val_ds.map(lambda x, y: (pre_processor(x), y))

        k_fold_dict[i] = {'train_ds': train_ds_with_preproc, 'val_ds': val_ds_with_preproc, 'weights': class_weight}

    return k_fold_dict


def cross_validation(k_f, pre_processor, inp):
    models_to_fit = []
    models_to_stack = []
    evaluation = {}

    for i in range(len(k_f.keys())):
        models_to_fit.append(tfdf.keras.GradientBoostedTreesModel(max_depth=7, num_trees=1000, random_seed=i))

    for i, m_to_fit in enumerate(models_to_fit):
        k_fold_model_pred = m_to_fit(pre_processor(inp))
        models_to_stack.append(k_fold_model_pred)

    mean_gbt = tf.reduce_mean(tf.stack(models_to_stack, axis=0), axis=0)
    ensemble_gbt = tf.keras.models.Model(inputs=inp, outputs=mean_gbt)

    for i, m_to_fit in enumerate(models_to_fit):
        m_to_fit.fit(k_f[i]['train_ds'], class_weight=k_f[i]['weights'])

        # evaluation
        m_to_fit.compile(["AUC"])
        m_to_fit_eval = m_to_fit.evaluate(k_f[i]['val_ds'], return_dict=True)

        evaluation[i] = {'eval': m_to_fit_eval}
    return ensemble_gbt, evaluation
```

<span style="float:right; font-size:70%"><a href="#top">Back to top</a></span>

#### **Build model**

This step is meant to perform cross-validation multiple times in order to build models and
evaluate their performance according to different hyperparameters configurations.

**_Key takeaways_** : GBT model performs best when using 10^3 trees of maximum depth 7.

```python
# k-fold cross-validation
k_folds = k_fold(df, smoking, preprocessor, 10)

# build model
# GBT: max_depth=7, num_trees=1000
ensemble_model, _ = cross_validation(k_folds, preprocessor, inputs)

ensemble_model.compile(metrics=["AUC"])
ensemble_model.evaluate(test_ds)
```

<span style="float:right; font-size:70%"><a href="#top">Back to top</a></span>

#### **Make predictions**

The last step is making predictions for the unseen data and store them in a csv file.  

```python
# prediction
# read test data
test_file = 'playground/3.24/files/playground-series-s3e24/test.csv'
test_df = pd.read_csv(test_file)

test_smoker_id = test_df.pop('id')

test_ds = tf.data.Dataset.from_tensor_slices(dict(test_df))
test_ds = test_ds.batch(batch_size=32)

# predict
predictions = ensemble_model.predict(test_ds)
predictions = np.squeeze(predictions)

# write to csv
predictions_df = pd.concat([test_smoker_id, pd.Series(predictions)], axis=1)
predictions_df.to_csv(
    path_or_buf='playground/3.24/files/submission.csv',
    header=['id', 'smoking'],
    index=False
)
```

<span style="float:right; font-size:70%"><a href="#top">Back to top</a></span>

[^1]: Source files are available [here](https://www.kaggle.com/competitions/playground-series-s3e24/data)